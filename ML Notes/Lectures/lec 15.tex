\chapter{Lec 15 - Bayesian Learning I}

\section{Bayesian Methods}
Bayesian methods provide computational techniques of learning, but they are also useful for the interpretation/analysis of non-probabilistic algorithms.\newline\newline
These methods refer to set of statistical techniques for building models and making predictions based on the Bayesian framework. The observed training examples increase or decrease the probability that a hypothesis is correct.\newline\newline
The fundamental formula for Bayesian methods is the following:
\[P(h|D) = \frac{P(D|h)P(h)}{P(D)}\]
where:
\begin{itemize}
    \item $P(h):$ a priori probability of the hypothesis $h$
    \item $P(D):$ a priori probability of training data. It is the probability to observe exactly this training set when we don't know anything about the hypothesis.
    \item $P(h|D):$ probability of $h$ given $D$. It is the probability that $h$ is the hypothesis that generates data $D$.
    \item $P(D|h):$ probability if $D$ given $h$. Given a hypothesis $h$, it is the probability of data $D$ to be generated by $h$.
\end{itemize}
In general, we want to select the most probable hypothesis given the learning data, known as \textbf{maximum a posterior hypothesis} $h_{MAP}:$
\[h_{MAP} = argmax_{h \in H}P(h|D)\]
\[= argmax_{h \in H}\frac{P(D|h)P(h)}{P(D)}\]
Since $P(D)$ does not depend on $h$, we can consider it as a constant and remove it from the equation.
\[= argmax_{h \in H}P(D|h)P(h)\]
If we assume uniform probabilities on the hypotheses, that is $P(h_{i}) = P(h_{j})$, we can choose the so called \textbf{maximum likelihood hypothesis} $h_{ML}:$
\[h_{ML} = argmax_{h \in H}P(D|h)\]
How can we learn maximum a posterior hypothesis $h_{MAP}$? A very simple algorithm can be the \textit{brute force} method, in which, for each hypothesis $h \in H$, it computes $h_{MAP}$ and returns the hypothesis with the highest probability. Obviously, this approach cannot be implemented since the hypothesis space contains too many hypothesis (can also be infinite).
\subsection{Learning of a real-valued function}
Consider any real-valued target function $f$ and learning examples $\langle 
 \textbf{x}_{i}, d_{i}\rangle$ where $d_{i}$ has some noise:
 \begin{itemize}
     \item $d_{i} = f(\textbf{x}_{i}) + e_{i}$
     \item $e_{i}$ is a random variable (noise) extracted independently, for each $\textbf{x}_{i}$, according to a Gaussian distribution with mean 0.
 \end{itemize}
 It can be shown that the hypothesis $h_{ML}$ (maximum likelihood) is the one that minimizes the mean squared error:
 \[h_{ML} = argmin_{h \in H} \sum_{i=0}^{m}(d_{i} - h(\textbf{x}_{i}))^{2}\]
 Basically, the solution of the minimization of the MSE can be motivated by finding the maximum likelihood hypothesis.

\subsection{Learning a hypothesis that predicts a probability}
For what concerning the classification task, the usual scenario is to predict a class according to the input data. How can we learn a hypothesis that predicts a probability ? \newline\newline
Consider the scenario of a probabilistic function $f: X \rightarrow \{0, 1\}$. $X$ might represent medical patients in terms of their symptoms and $f(x)$ might be 1 if the patient survives and 0 if not. We want to learn a predictor (e.g. a neural network) $f^{'} \rightarrow [0, 1]$ which predicts the probability that $f(x) = 1$ given $x$.\newline\newline
Given the input data $D = \{(\textbf{x}_{1}, d_{1}), ..., (\textbf{x}_{n}, d_{n})\}$, it can be shown that the criterion we should optimize to find a maximum likelihood hypothesis for $f^{'}$ is the cross-entropy loss function:
\[argmax_{h \in H} \sum_{i = 1}^{m} d_{i} ln(h(\textbf{x}_{i})) + (1 - d_{i}) ln(1 - h(\textbf{x}_{i}))\]

 \section{Most likely classification for new instances}
 Given a new instance $\textbf{x}$, which is the most likely \textbf{classification}?
 The classification given by the most likely hypothesis $h_{MAP}$ is not necessarily the most likely classification. For example, given the following three possible hypothesis:
 \[P(h_{1} | D) = 0.4 \quad P(h_{2} | D) = 0.3 \quad P(h_{3} | D) = 0.3\]
 We want to classify a new instance $\textbf{x}$:
 \[h_{1}(\textbf{x}) = + \quad h_{2}(\textbf{x}) = - \quad h_{1}(\textbf{x}) = -\]
 The most likely hypothesis $h_{1}$ classifies $\textbf{x}$ with the label (+), but the most likely classification is (-). This is because the optimal (Bayes) classification of a certain instance is the class $v_{j} \in V$ which maximizes the following probability:
 \[argmax_{v_{j} \in V} = \sum_{h_{i} \in H}P(v_{j} | h_{i})P(h_{i} | D)\]
 where $V$ is the set of possible labels.\newline\newline
 Computing this optimal classification can be very expensive if there are many hypothesis. A method to overcome this problem can be the Gibbs algorithm:
 \begin{enumerate}
     \item Choose a hypothesis at random, with probability $P(h | D)$
     \item Use it to classify the new instance
 \end{enumerate}
The surprising fact is that if we assume that the target concepts are randomly extracted from $H$ according to an a priori probability on $H$, it follows that:
\[E[\epsilon_{Gibbs}] \leq 2E[\epsilon_{Bayes}]\]
By repeating the algorithm multiple times, The error of Gibbs sampling tends to be less than the expected value of bayes error. 
